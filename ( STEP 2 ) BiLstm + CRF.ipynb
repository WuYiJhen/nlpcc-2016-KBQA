{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立 BiLSTM + CRF 模型訓練命名實體識別"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a0972\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "#let the gpu allocates memory space dynamically\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Bidirectional, Dense, TimeDistributed, Dropout\n",
    "from keras_contrib.layers.crf import CRF\n",
    "from keras_contrib.utils import save_load_utils\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('04_NER_training_data.csv')\n",
    "training_data['question'] = training_data['question'].apply(lambda x : eval(x))\n",
    "training_data['tag'] = training_data['tag'].apply(lambda x : eval(x))\n",
    "training_data['purpose'] = training_data['purpose'].apply(lambda x : eval(x))\n",
    "\n",
    "\n",
    "testing_data = pd.read_csv('04_NER_testing_data.csv')\n",
    "testing_data = testing_data.apply(lambda x : x.apply(lambda y : eval(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4057 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "all_data = pd.concat([training_data['question'], testing_data['question']])\n",
    "tokenizer_X = Tokenizer()\n",
    "tokenizer_X.fit_on_texts(all_data)\n",
    "word_index_X = tokenizer_X.word_index\n",
    "print('Found %s unique tokens.' % len(word_index_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = all_data.apply(lambda x : len(x)).max()\n",
    "X = tokenizer_X.texts_to_sequences(training_data['question'])\n",
    "X = pad_sequences(X, maxlen = MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_Y = Tokenizer()\n",
    "tokenizer_Y.fit_on_texts(training_data['tag'])\n",
    "word_index_Y = tokenizer_Y.word_index\n",
    "print('Found %s unique tokens.' % len(word_index_Y))\n",
    "Y = tokenizer_Y.texts_to_sequences(training_data['tag'])\n",
    "Y = pad_sequences(Y, maxlen = MAX_SEQUENCE_LENGTH)\n",
    "Y = np.expand_dims(Y,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 建立兩層的 BiLSTM ，最後再接上 CRF Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a0972\\Anaconda3\\lib\\site-packages\\keras_contrib-2.0.8-py3.6.egg\\keras_contrib\\layers\\crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
      "C:\\Users\\a0972\\Anaconda3\\lib\\site-packages\\keras_contrib-2.0.8-py3.6.egg\\keras_contrib\\layers\\crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 58, 100)           361400    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 58, 100)           60400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 58, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 58, 100)           60400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 58, 100)           0         \n",
      "_________________________________________________________________\n",
      "crf_1 (CRF)                  (None, 58, 4)             428       \n",
      "=================================================================\n",
      "Total params: 482,628\n",
      "Trainable params: 482,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(4057, 100, input_length=X.shape[1], trainable=True))\n",
    "\n",
    "model.add(Bidirectional(LSTM(50, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Bidirectional(LSTM(50, return_sequences=True)))\n",
    "model.add(Dropout(0.1))\n",
    "crf_layer = CRF(4, sparse_target=True)\n",
    "model.add(crf_layer)\n",
    "optimizer = optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0001, amsgrad=False)\n",
    "model.compile(loss = crf_layer.loss_function, optimizer = optimizer, metrics=[crf_layer.accuracy])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nepochs = 50\\n\\nbatch_size = 100\\n\\nweight_save = ModelCheckpoint('04_NER_weight.hdf5', save_best_only=True, monitor='val_loss', mode='min')\\n\\nmodel.fit(X, Y, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience = 2, min_delta=0.0001), weight_save])\\n\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "weight_save = ModelCheckpoint('04_NER_weight.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "model.fit(X, Y, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience = 2, min_delta=0.0001), weight_save])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = tokenizer_X.texts_to_sequences(testing_data['question'])\n",
    "padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "model.load_weights('04_NER_weight.hdf5')\n",
    "pred = model.predict(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Entity = []\n",
    "Purpose = []\n",
    "\n",
    "for i in range(len(testing_data['question'])) :\n",
    "    \n",
    "    prediction = np.argmax(pred[i], axis = 1)[-len(testing_data['question'][i]):]\n",
    "    \n",
    "    Entity.append(''.join(np.array(testing_data['question'][i])[prediction >= 2].tolist()))\n",
    "    \n",
    "    Purpose.append(np.array(testing_data['question'][i])[prediction == 1].tolist())\n",
    "    \n",
    "    \n",
    "testing_data['entity'] = Entity\n",
    "testing_data['purpose'] = Purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>entity</th>\n",
       "      <th>purpose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[你, 知, 道, 计, 算, 机, 应, 用, 基, 础, 这, 本, 书, 的, 作, ...</td>\n",
       "      <td>计算机应用基础</td>\n",
       "      <td>[你, 知, 道, 这, 本, 书, 的, 作, 者, 是, 谁, 吗, ？]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[计, 算, 机, 应, 用, 基, 础, 这, 本, 书, 的, 出, 版, 社, 是, ...</td>\n",
       "      <td>计算机应用基础</td>\n",
       "      <td>[这, 本, 书, 的, 出, 版, 社, 是, 那, 个, ？]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[告, 诉, 我, 高, 等, 数, 学, 的, 出, 版, 时, 间, 是, 什, 么, ...</td>\n",
       "      <td>高等数学</td>\n",
       "      <td>[告, 诉, 我, 的, 出, 版, 时, 间, 是, 什, 么, 时, 候, ？]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[我, 想, 知, 道, 戴, 维, 斯, 是, 什, 么, 国, 家, 的, 人, ？]</td>\n",
       "      <td>戴维斯</td>\n",
       "      <td>[我, 想, 知, 道, 是, 什, 么, 国, 家, 的, 人, ？]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[你, 知, 道, 高, 等, 数, 学, 的, i, s, b, n, 吗, ？]</td>\n",
       "      <td>高等数学</td>\n",
       "      <td>[你, 知, 道, 的, i, s, b, n, 吗, ？]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question   entity  \\\n",
       "0  [你, 知, 道, 计, 算, 机, 应, 用, 基, 础, 这, 本, 书, 的, 作, ...  计算机应用基础   \n",
       "1  [计, 算, 机, 应, 用, 基, 础, 这, 本, 书, 的, 出, 版, 社, 是, ...  计算机应用基础   \n",
       "2  [告, 诉, 我, 高, 等, 数, 学, 的, 出, 版, 时, 间, 是, 什, 么, ...     高等数学   \n",
       "3      [我, 想, 知, 道, 戴, 维, 斯, 是, 什, 么, 国, 家, 的, 人, ？]      戴维斯   \n",
       "4         [你, 知, 道, 高, 等, 数, 学, 的, i, s, b, n, 吗, ？]     高等数学   \n",
       "\n",
       "                                      purpose  \n",
       "0     [你, 知, 道, 这, 本, 书, 的, 作, 者, 是, 谁, 吗, ？]  \n",
       "1           [这, 本, 书, 的, 出, 版, 社, 是, 那, 个, ？]  \n",
       "2  [告, 诉, 我, 的, 出, 版, 时, 间, 是, 什, 么, 时, 候, ？]  \n",
       "3        [我, 想, 知, 道, 是, 什, 么, 国, 家, 的, 人, ？]  \n",
       "4              [你, 知, 道, 的, i, s, b, n, 吗, ？]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data.to_csv('04_Entity_testing_data.csv', index = False, encoding='utf_8_sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
